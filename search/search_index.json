{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome The software renderer project is a 3D renderer created as a personal project and coded in C. The software renderer program replicates computations done by the GPU hardware inside a C program. This project was made to challenge myself to write a software rasterizer from scratch with little prior knowledge in C and 3D mathematics. Learning graphics concepts and math in the process. It is a learning journey, so as much as possible, it will use handmade solutions (memory containers, maths library, 3D scene management, 3D graphics abstraction, OS interactions). Features 3D mesh rasterization on the CPU. Line renderer. Pixel shading with texture mapping. Flat shading. 3D transformation gizmo to move entities in the world. obj file loader. The big picture of the algorithms used for the software renderer and the entity movement are detailed here. Third party Usage of third party libraries is limited to : stbimage for png loading. Entity selection Click one of the cube to select it. Press T,R or S to enable translation, rotation and scale gizmos. Try it from precompiled binaries Download and extract the binaries . Or build from source The project uses CMake as build system. Clone the repository and on the root folder with power shell : mkdir ./build cd ./build cmake ../ cmake --build . --target EntitySelectionTest It will generate EntitySelectionTest.exe in the path mentioned in the command prompt. Software renderer shocase Try it from precompiled binaries Download and extract the binaries . Or build from source The project uses CMake as build system. Clone the repository and on the root folder with power shell : mkdir ./build cd ./build cmake ../ cmake --build . --target SoftwareRendererShowcase_icosphere cmake --build . --target SoftwareRendererShowcase_tower It will generate SoftwareRendererShowcase_icosphere.exe and SoftwareRendererShowcase_tower.exe in the path mentioned in the command prompt. WARNING: if you want to run the program from source, make sure to update the EXECUTABLEPATH_DEV definition in the source code to make it point to the project root path that contains the asset folder.","title":"Welcome"},{"location":"#welcome","text":"The software renderer project is a 3D renderer created as a personal project and coded in C. The software renderer program replicates computations done by the GPU hardware inside a C program. This project was made to challenge myself to write a software rasterizer from scratch with little prior knowledge in C and 3D mathematics. Learning graphics concepts and math in the process. It is a learning journey, so as much as possible, it will use handmade solutions (memory containers, maths library, 3D scene management, 3D graphics abstraction, OS interactions).","title":"Welcome"},{"location":"#features","text":"3D mesh rasterization on the CPU. Line renderer. Pixel shading with texture mapping. Flat shading. 3D transformation gizmo to move entities in the world. obj file loader. The big picture of the algorithms used for the software renderer and the entity movement are detailed here.","title":"Features"},{"location":"#third-party","text":"Usage of third party libraries is limited to : stbimage for png loading.","title":"Third party"},{"location":"#entity-selection","text":"Click one of the cube to select it. Press T,R or S to enable translation, rotation and scale gizmos.","title":"Entity selection"},{"location":"#try-it-from-precompiled-binaries","text":"Download and extract the binaries .","title":"Try it from precompiled binaries"},{"location":"#or-build-from-source","text":"The project uses CMake as build system. Clone the repository and on the root folder with power shell : mkdir ./build cd ./build cmake ../ cmake --build . --target EntitySelectionTest It will generate EntitySelectionTest.exe in the path mentioned in the command prompt.","title":"Or build from source"},{"location":"#software-renderer-shocase","text":"","title":"Software renderer shocase"},{"location":"#try-it-from-precompiled-binaries_1","text":"Download and extract the binaries .","title":"Try it from precompiled binaries"},{"location":"#or-build-from-source_1","text":"The project uses CMake as build system. Clone the repository and on the root folder with power shell : mkdir ./build cd ./build cmake ../ cmake --build . --target SoftwareRendererShowcase_icosphere cmake --build . --target SoftwareRendererShowcase_tower It will generate SoftwareRendererShowcase_icosphere.exe and SoftwareRendererShowcase_tower.exe in the path mentioned in the command prompt. WARNING: if you want to run the program from source, make sure to update the EXECUTABLEPATH_DEV definition in the source code to make it point to the project root path that contains the asset folder.","title":"Or build from source"},{"location":"ecs/","text":"The ECS engine is a functional object that stores entities and their associated components. Architecture Entity An Entity by itself doesn't do anything, it is a bag of components. To be fully defined and interact with the world, it must have components attached to it. Component In the point of view of the ECS engine, components are a type with a pointer to an externally allocated resource. When we allocate a component, the external ressource has already been allocated, we provide a pointer to it so that can be retrieved back in the system. Global events All operations on the ECS engine or components are not executed immediately, instead, they are pushed to the ECSGlobalEvent stack. These events must be consumed before any system are executed. The events are : Entity allocation/release: Entities are pushed or removed from the to the entity container once the event is consumed. On entity release, the entity components are scanned and the registered entity filter are notified. Component added/removed: Upon consumption of component events, registered entity filters are notified if type is matching. Entity filter The Entity engine doesn't hold any logic based on components. Instead, the ECS engine allows it's consumer to filter entities by their attached component types. These components events are triggered when a specified component type is added or removed to an entity. These views are called entity filters. For example, an entity filter is set to listen to event of component types A and B. Once a node is allocated and a component of type A is attached to it, the entity filter doesn't push the entity because it needs to have component A and B. It is when the component of type B is attached to it that the entity is pushed to the filter. The same logic is applied when a component is detached. System Systems are the interface between the ECS and the internal systems. A system usually holds one or more entity filters. By consuming filter events, the system can build arrays of entity that are ensured to have the required component to work with.","title":"Ecs"},{"location":"ecs/#architecture","text":"","title":"Architecture"},{"location":"ecs/#entity","text":"An Entity by itself doesn't do anything, it is a bag of components. To be fully defined and interact with the world, it must have components attached to it.","title":"Entity"},{"location":"ecs/#component","text":"In the point of view of the ECS engine, components are a type with a pointer to an externally allocated resource. When we allocate a component, the external ressource has already been allocated, we provide a pointer to it so that can be retrieved back in the system.","title":"Component"},{"location":"ecs/#global-events","text":"All operations on the ECS engine or components are not executed immediately, instead, they are pushed to the ECSGlobalEvent stack. These events must be consumed before any system are executed. The events are : Entity allocation/release: Entities are pushed or removed from the to the entity container once the event is consumed. On entity release, the entity components are scanned and the registered entity filter are notified. Component added/removed: Upon consumption of component events, registered entity filters are notified if type is matching.","title":"Global events"},{"location":"ecs/#entity-filter","text":"The Entity engine doesn't hold any logic based on components. Instead, the ECS engine allows it's consumer to filter entities by their attached component types. These components events are triggered when a specified component type is added or removed to an entity. These views are called entity filters. For example, an entity filter is set to listen to event of component types A and B. Once a node is allocated and a component of type A is attached to it, the entity filter doesn't push the entity because it needs to have component A and B. It is when the component of type B is attached to it that the entity is pushed to the filter. The same logic is applied when a component is detached.","title":"Entity filter"},{"location":"ecs/#system","text":"Systems are the interface between the ECS and the internal systems. A system usually holds one or more entity filters. By consuming filter events, the system can build arrays of entity that are ensured to have the required component to work with.","title":"System"},{"location":"ecs_types/","text":"This page describe all component and system types. Transform component The transform component defines a 3D position of the entity. When updating an entity transform, what we change is the local transform. The local to world matrix is recalculated on the fly, only when needed. The local to world matrix value is cached until the transform change again. Because transforms have a parent/child relationship, the parent movement affect the child position. When a transform parent position is set, this change is propagated recursively into the children. This propagation is done instantaneously. When a transform has changed, the internal state of the transform indicates that the node has changed. This state can be consumed to check if a node has moved or not. MeshRenderer component The MeshRenderer component holds references to a render material object and mesh resource. Material and mesh are allocated instantaneously on demand. MeshRenderer also computes the bounding box the the allocated mesh. Mesh draw system The mesh draw system updates render model matrices when an entity has moved. ECS event consumption: The system allocate an entity filter that listen to the presence of Transform and MeshRenderer components. When an entity met the criteria, the system allocates a rendered object from the render system and link it to the mesh renderer component. The rendered object is deallocated when criteria are no more met. Camera component The Camera component holds projection and view matrices. Camera render system The camera render system updates view and projection matrices when an entity has moved. ECS event consumption: The system allocate an entity filter that listen to the presence of Transform and Camera components. PhysicsBody component The PhysicsBody component holds a reference to : The BoxCollider object of the physics system. The transform component to get entity position. The mesh renderer bounding box to get the shape geometry. Physics system ECS event consumption: The system allocate an entity filter that listen to the presence of Transform, PhysicsBody and MeshRenderer components. When an entity met the criteria, the system allocates a box collider object and push it to the physics world.","title":"Ecs types"},{"location":"ecs_types/#transform-component","text":"The transform component defines a 3D position of the entity. When updating an entity transform, what we change is the local transform. The local to world matrix is recalculated on the fly, only when needed. The local to world matrix value is cached until the transform change again. Because transforms have a parent/child relationship, the parent movement affect the child position. When a transform parent position is set, this change is propagated recursively into the children. This propagation is done instantaneously. When a transform has changed, the internal state of the transform indicates that the node has changed. This state can be consumed to check if a node has moved or not.","title":"Transform component"},{"location":"ecs_types/#meshrenderer-component","text":"The MeshRenderer component holds references to a render material object and mesh resource. Material and mesh are allocated instantaneously on demand. MeshRenderer also computes the bounding box the the allocated mesh.","title":"MeshRenderer component"},{"location":"ecs_types/#mesh-draw-system","text":"The mesh draw system updates render model matrices when an entity has moved. ECS event consumption: The system allocate an entity filter that listen to the presence of Transform and MeshRenderer components. When an entity met the criteria, the system allocates a rendered object from the render system and link it to the mesh renderer component. The rendered object is deallocated when criteria are no more met.","title":"Mesh draw system"},{"location":"ecs_types/#camera-component","text":"The Camera component holds projection and view matrices.","title":"Camera component"},{"location":"ecs_types/#camera-render-system","text":"The camera render system updates view and projection matrices when an entity has moved. ECS event consumption: The system allocate an entity filter that listen to the presence of Transform and Camera components.","title":"Camera render system"},{"location":"ecs_types/#physicsbody-component","text":"The PhysicsBody component holds a reference to : The BoxCollider object of the physics system. The transform component to get entity position. The mesh renderer bounding box to get the shape geometry.","title":"PhysicsBody component"},{"location":"ecs_types/#physics-system","text":"ECS event consumption: The system allocate an entity filter that listen to the presence of Transform, PhysicsBody and MeshRenderer components. When an entity met the criteria, the system allocates a box collider object and push it to the physics world.","title":"Physics system"},{"location":"entity_editor_selection/","text":"The entity editor selection is a system implemented to assert that the math library is correctly implemented. The system allows : Select any entity with a mesh renderer component by clicking on it. Move, rotate and scale entity with 3D gizmos. Entity selection The entity selection is done by casting a ray from the camera transform position in the direction pointed by the mouse against the physics world. The first intersected physics body bounding box is the entity that will be selected. Entity movement The entity movement system is the most basic form of moving a 3D object in a scene with gizmo implemented by any 3D software. The movement algorithm is composed of three parts : 1. The guiding plane A guiding plane is either instantiated or positioned at the selected transform. The guiding plane will be used to project the screen space movement of the mouse in world space. 2. Mouse movement projection The mouse movement delta between two frames is projected from the screen space to the guiding plane. Projection is done by finding the intersection points between the rays fired to mouse positions from the camera. All further calculations are based on this projection. 3. transform movement calculation Translation: The translation gizmo is represented by three arrows that points to the transform local axis. For one selected direction, the guiding plane must contain the line defined by the direction. Also, to allow the user to move the object, the plane orientation is facing \"as much as possible\" the camera. This means that the angle between the guiding plane normal and the camera direction must be minimal. The delta mouse position in plane space is projected to the selected axis. Resulting to the movement distance of the transform along the selected direction. Rotation: The rotation gizmo is represented by three quarter circle facing the orientation axis. The guiding plane is facing the camera. The delta mouse position is converted to angle delta by taking the transform as point of origin. The angle delta is applied to the rotation. This could have been entirely be done in screen space. Scale: Exact same as the translation, but instead of moving the transform, we scale it.","title":"Entity editor selection"},{"location":"entity_editor_selection/#entity-selection","text":"The entity selection is done by casting a ray from the camera transform position in the direction pointed by the mouse against the physics world. The first intersected physics body bounding box is the entity that will be selected.","title":"Entity selection"},{"location":"entity_editor_selection/#entity-movement","text":"The entity movement system is the most basic form of moving a 3D object in a scene with gizmo implemented by any 3D software. The movement algorithm is composed of three parts :","title":"Entity movement"},{"location":"entity_editor_selection/#1-the-guiding-plane","text":"A guiding plane is either instantiated or positioned at the selected transform. The guiding plane will be used to project the screen space movement of the mouse in world space.","title":"1. The guiding plane"},{"location":"entity_editor_selection/#2-mouse-movement-projection","text":"The mouse movement delta between two frames is projected from the screen space to the guiding plane. Projection is done by finding the intersection points between the rays fired to mouse positions from the camera. All further calculations are based on this projection.","title":"2. Mouse movement projection"},{"location":"entity_editor_selection/#3-transform-movement-calculation","text":"Translation: The translation gizmo is represented by three arrows that points to the transform local axis. For one selected direction, the guiding plane must contain the line defined by the direction. Also, to allow the user to move the object, the plane orientation is facing \"as much as possible\" the camera. This means that the angle between the guiding plane normal and the camera direction must be minimal. The delta mouse position in plane space is projected to the selected axis. Resulting to the movement distance of the transform along the selected direction. Rotation: The rotation gizmo is represented by three quarter circle facing the orientation axis. The guiding plane is facing the camera. The delta mouse position is converted to angle delta by taking the transform as point of origin. The angle delta is applied to the rotation. This could have been entirely be done in screen space. Scale: Exact same as the translation, but instead of moving the transform, we scale it.","title":"3. transform movement calculation"},{"location":"software_renderer/","text":"The software renderer is the algorithm that draw 3D objects on the screen without using the GPU. This means that all graphics pipeline steps must be implemented from scratch, from colouring pixels to object culling. The renderer supports : Object culling. Polygon culling. Polygon clipping. Polygon rasterization. Polygon perspective interpolation. Pixel depth buffer test. Pixel texture mapping Pixel shading. The whole algorithm consists of breaking objects to polygons to pixels by filtering the maximum number of data for each steps. The algorithm can be divided in three steps : Breaking objects to polygons. Breaking polygons to pixels. Pixel shading. Data model Material: The rendered color of the object with an optional texture. The material is used in the pixel shading stage. Materials can be typed to be assigned to different render pipeline : NotShaded_NotTextured NotShaded_Textured FlatShaded_Textured FlatShaded_NotTextured Flat shaded means that polygon faces act as if they are flat surface. RederedObject: The rendered object is the 3D object definition, it contains all informations to provide to the render pipeline to draw a 3D object. Render pipeline A render pipeline is the algorithm that draw 3D objects on a target texture. For every pipeline, all steps are the same, but the pixel shading step may differ depending of the material type. Is there texture mapping ? Do we need perspective interpolation ? Is flat shade enabled ? Different answers to these questions will produce different render pipeline. Even if this imply some small code duplication sometimes. This choice has been made to : Avoid branching inside inside pipeline as mush as possible. All pipelines are straightforward, more readable and performance problems may be easy to find and fix. Having no performance impact of introducing a new render pipeline. Every render pipeline take the following inputs : A camera with view matrix, projection matrix and precomputed frustum. A depth texture buffer. The target rendered texture. An array of rendered objects. Simplifications We assume that polygons are always triangles. This makes the polygon rasterization and interpolation straightforward. The render texture is defined by floating point and is the target texture used as output of the renderer. 1. Breaking objects to polygons 1.1 Camera space projection Every rendered objects positions are projected into camera space by using the rendered object model matrix and the camera view matrix. 1.2 Object culling Rendered objects are filtered by checking if the mesh bounding sphere intersects with the camera frustum. Once the bounding sphere is projected in camera space, we create three segments along the X,Y,Z (camera local space) and we check if each segments intersect or are contained within the camera frustum planes. If at least one segment is not, then the entire object is ensured to not be visible, so it is discarded. 1.3 Objects break to polygons For every objects, we extract all polygons and vertices from the mesh. Because every polygons are unique and may share the same vertices, camera and world positions of vertices are calculated only if they haven't been calculated already, this avoids to do the same matrix projection multiple times. 1.4 Polygon backface culling Only one side of polygons are rendered. To avoid pushing polygons that will not be rendered all polygons that are not facing the camera are discarded. This is done by calculating the oriented normal of the polygon and project it on the camera forward direction. 2. Breaking polygons to pixels 2.1 Polygon clipping The polygon clipping allows us to remove polygons that are beyond the camera. Projecting a point beyond the camera in clip space will produce garbage values so we filter them. This is done by ensuring that for every polygons in camera space, the forward (Z axis) value of every vertices are > 0. WARNING: This solution makes polygons popping in and out when they move across the camera. A smarter solution would be to cut the polygon with the near plane and discard only the cutted part that is behind the camera. 2.2 Polygon sorting Polygons are sorted by their distance from camera to render them from near to far. This step is required to avoid overdrawing as much as possible. Polygons near to the camera are more likely to hide others. 2.3 Rasterization Once the polygon has been projected to screen space, the rasterization function tells us if one pixel is inside the polygon. The rasterization can be divided in three step. Polygon bounding box calculation: To avoid to check all pixel of the rendered texture, we calculate the bounding box of the polygon that will discard all other pixels of the iteration. Polygon edge function check: To know if the pixel is inside the polygon, we calculate the edge function relative to the pixel for the three edges of the polygon. The edge function calculates the signed area of the parallelogram defined by the edge and pixel point. If all areas are positive, then this means that the pixel is inside. Interpolation factors: We want to be able to interpolate data associated to vertices to all pixels. An interpolated value for a pixel is defined as (V1 * I1) + (V2 * I2) + (V3 * I3) where V1, V2 and V3 are the datas associated to vertices. I1, I2 and I3 are the interpolation factors. For vertex V1, the interpolation factor is the ratio between the area calculated by the edge function relative to the (V2-V3) edge and the pixel and the area calculated by the edge function relative to the (V2-V3) edge and the V1 position. If the pixel is positioned at V1, then I1 will be 1 and I2 and I3 will be zero because their areas is shrank to a segment. 2.4 Depth testing Multiple polygon can write to the same pixel. We must be sure that the final pixel color corresponds to the pixel that is the nearest from the camera. To do so, we interpolate vertices depth along the polygon and compare it's value against a depth texture that have the same size as the render target. If the pixel pass the test, it's depth is written to the depth texture. Using a linear interpolation with the interpolation factors will lead to distortion. Because the depth no longer exist in the render target texture, a pixel with X,Y coordinate will be interpolated the same way if it's depth is 0.5 or 1. To take the perspective into account, we must linearly interpolate the value with a 1.0f/z factor. 3 Pixel shading The pixel shading will write the calculated color of every pixel to the render target texture. Color inputs are provided by the material of the rendered object. 3.1 Texture mapping Texture mapping is done by assigning uv coordinates to vertices. So by using the perspective interpolation, we get UV coordinates per pixels and request back the texture. No filtering is applied. Resources polygon rasterization perspective-correct interpolation","title":"Software Renderer"},{"location":"software_renderer/#data-model","text":"Material: The rendered color of the object with an optional texture. The material is used in the pixel shading stage. Materials can be typed to be assigned to different render pipeline : NotShaded_NotTextured NotShaded_Textured FlatShaded_Textured FlatShaded_NotTextured Flat shaded means that polygon faces act as if they are flat surface. RederedObject: The rendered object is the 3D object definition, it contains all informations to provide to the render pipeline to draw a 3D object.","title":"Data model"},{"location":"software_renderer/#render-pipeline","text":"A render pipeline is the algorithm that draw 3D objects on a target texture. For every pipeline, all steps are the same, but the pixel shading step may differ depending of the material type. Is there texture mapping ? Do we need perspective interpolation ? Is flat shade enabled ? Different answers to these questions will produce different render pipeline. Even if this imply some small code duplication sometimes. This choice has been made to : Avoid branching inside inside pipeline as mush as possible. All pipelines are straightforward, more readable and performance problems may be easy to find and fix. Having no performance impact of introducing a new render pipeline. Every render pipeline take the following inputs : A camera with view matrix, projection matrix and precomputed frustum. A depth texture buffer. The target rendered texture. An array of rendered objects.","title":"Render pipeline"},{"location":"software_renderer/#simplifications","text":"We assume that polygons are always triangles. This makes the polygon rasterization and interpolation straightforward. The render texture is defined by floating point and is the target texture used as output of the renderer.","title":"Simplifications"},{"location":"software_renderer/#1-breaking-objects-to-polygons","text":"","title":"1. Breaking objects to polygons"},{"location":"software_renderer/#11-camera-space-projection","text":"Every rendered objects positions are projected into camera space by using the rendered object model matrix and the camera view matrix.","title":"1.1 Camera space projection"},{"location":"software_renderer/#12-object-culling","text":"Rendered objects are filtered by checking if the mesh bounding sphere intersects with the camera frustum. Once the bounding sphere is projected in camera space, we create three segments along the X,Y,Z (camera local space) and we check if each segments intersect or are contained within the camera frustum planes. If at least one segment is not, then the entire object is ensured to not be visible, so it is discarded.","title":"1.2 Object culling"},{"location":"software_renderer/#13-objects-break-to-polygons","text":"For every objects, we extract all polygons and vertices from the mesh. Because every polygons are unique and may share the same vertices, camera and world positions of vertices are calculated only if they haven't been calculated already, this avoids to do the same matrix projection multiple times.","title":"1.3 Objects break to polygons"},{"location":"software_renderer/#14-polygon-backface-culling","text":"Only one side of polygons are rendered. To avoid pushing polygons that will not be rendered all polygons that are not facing the camera are discarded. This is done by calculating the oriented normal of the polygon and project it on the camera forward direction.","title":"1.4 Polygon backface culling"},{"location":"software_renderer/#2-breaking-polygons-to-pixels","text":"","title":"2. Breaking polygons to pixels"},{"location":"software_renderer/#21-polygon-clipping","text":"The polygon clipping allows us to remove polygons that are beyond the camera. Projecting a point beyond the camera in clip space will produce garbage values so we filter them. This is done by ensuring that for every polygons in camera space, the forward (Z axis) value of every vertices are > 0. WARNING: This solution makes polygons popping in and out when they move across the camera. A smarter solution would be to cut the polygon with the near plane and discard only the cutted part that is behind the camera.","title":"2.1 Polygon clipping"},{"location":"software_renderer/#22-polygon-sorting","text":"Polygons are sorted by their distance from camera to render them from near to far. This step is required to avoid overdrawing as much as possible. Polygons near to the camera are more likely to hide others.","title":"2.2 Polygon sorting"},{"location":"software_renderer/#23-rasterization","text":"Once the polygon has been projected to screen space, the rasterization function tells us if one pixel is inside the polygon. The rasterization can be divided in three step. Polygon bounding box calculation: To avoid to check all pixel of the rendered texture, we calculate the bounding box of the polygon that will discard all other pixels of the iteration. Polygon edge function check: To know if the pixel is inside the polygon, we calculate the edge function relative to the pixel for the three edges of the polygon. The edge function calculates the signed area of the parallelogram defined by the edge and pixel point. If all areas are positive, then this means that the pixel is inside. Interpolation factors: We want to be able to interpolate data associated to vertices to all pixels. An interpolated value for a pixel is defined as (V1 * I1) + (V2 * I2) + (V3 * I3) where V1, V2 and V3 are the datas associated to vertices. I1, I2 and I3 are the interpolation factors. For vertex V1, the interpolation factor is the ratio between the area calculated by the edge function relative to the (V2-V3) edge and the pixel and the area calculated by the edge function relative to the (V2-V3) edge and the V1 position. If the pixel is positioned at V1, then I1 will be 1 and I2 and I3 will be zero because their areas is shrank to a segment.","title":"2.3 Rasterization"},{"location":"software_renderer/#24-depth-testing","text":"Multiple polygon can write to the same pixel. We must be sure that the final pixel color corresponds to the pixel that is the nearest from the camera. To do so, we interpolate vertices depth along the polygon and compare it's value against a depth texture that have the same size as the render target. If the pixel pass the test, it's depth is written to the depth texture. Using a linear interpolation with the interpolation factors will lead to distortion. Because the depth no longer exist in the render target texture, a pixel with X,Y coordinate will be interpolated the same way if it's depth is 0.5 or 1. To take the perspective into account, we must linearly interpolate the value with a 1.0f/z factor.","title":"2.4 Depth testing"},{"location":"software_renderer/#3-pixel-shading","text":"The pixel shading will write the calculated color of every pixel to the render target texture. Color inputs are provided by the material of the rendered object.","title":"3 Pixel shading"},{"location":"software_renderer/#31-texture-mapping","text":"Texture mapping is done by assigning uv coordinates to vertices. So by using the perspective interpolation, we get UV coordinates per pixels and request back the texture. No filtering is applied.","title":"3.1 Texture mapping"},{"location":"software_renderer/#resources","text":"polygon rasterization perspective-correct interpolation","title":"Resources"}]}